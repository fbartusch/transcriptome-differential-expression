from glob import glob
import os
from os import path
import pandas as pd
from collections import OrderedDict
import sys

localrules: merge_counts, write_coldata, write_de_params, de_analysis

include: "rules/utils.smk"

configfile: "config.yml"

SNAKEDIR = path.dirname(workflow.snakefile)
inputdir  = config["inputdir"]
resdir = config["resdir"]

condition_a_samples = glob(os.path.join(inputdir, "*_{identifier}*".format(identifier=config["condition_a_identifier"])))

condition_b_samples = glob(os.path.join(inputdir, "*_{identifier}*".format(identifier=config["condition_b_identifier"])))

all_samples = condition_a_samples + condition_b_samples
datasets = [path.basename(x).rsplit(".")[0] for x in all_samples]

rule all:
    input:
        ver = rules.dump_versions.output.ver,
        count_tsvs = expand("counts/{sample}_salmon/quant.sf", sample=datasets),
        merged_tsv = "merged/all_counts.tsv",
        coldata = "de_analysis/coldata.tsv",
        de_params = "de_analysis/de_params.tsv",
        res_dge = "de_analysis/results_dge.pdf",
        dtu_pdf = "de_analysis/dtu_plots.pdf",

rule build_minimap_index: ## build minimap2 index
    input:
        genome = config["transcriptome"]
    output:
        index = "index/transcriptome_index.mmi"
    params:
        opts = config["minimap_index_opts"]
    conda: "envs/env.yml"
    shell:"""
        minimap2 -t {resources.cpus_per_task} {params.opts} -d {output.index} {input.genome}
    """

#TODO: split into group rule, 'outsource' or combine samtools calls. 
#TODO: inquire suffix of input.fastq from files
rule map_reads: ## map reads using minimap2
    input:
       index = rules.build_minimap_index.output.index,
       fastq = os.path.join(config["inputdir"], "{sample}.fq.gz")
    output:
       bam = "alignments/{sample}.bam",
       sbam = "sorted_alignments/{sample}.bam",
    log: "logs/minimap2/{sample}.log"
    params:
        opts = config["minimap2_opts"],
        msec = config["maximum_secondary"],
        psec = config["secondary_score_ratio"],
    conda: "envs/env.yml"
    shell:"""
    (minimap2 -t {resources.threads} -ax map-ont -p {params.psec} -N {params.msec} {params.opts} {input.index} {input.fastq}\
    | samtools view -Sb > {output.bam};
    samtools sort -@ {threads} {output.bam} -o {output.sbam};
    samtools index {output.sbam};) 2> {log}
    """

rule count_reads:
    input:
        bam = rules.map_reads.output.bam,
        trs = config["transcriptome"],
    output:
        tsv = "counts/{sample}_salmon/quant.sf",
    params:
        tsv_dir = "counts/{sample}_salmon",
        libtype = config["salmon_libtype"],
    conda: "envs/env.yml"
    shell: """
        salmon quant --noErrorModel -p {resources.threads} -t {input.trs} -l {params.libtype} -a {input.bam} -o {params.tsv_dir}
    """

rule merge_counts:
    input:
        count_tsvs = expand("counts/{sample}_salmon/quant.sf", sample=datasets),
    output:
        "merged/all_counts.tsv"
    #conda: "envs/env.yml"
    script: 
        "scripts/merge_count_tsvs.py"

rule write_coldata:
    input:
    output:
        coldata = "de_analysis/coldata.tsv"
    run:
        samples, conditions, types = [], [], []
        for sample in condition_a_samples:
            samples.append(path.basename(sample).rsplit(".")[0])
            conditions.append(config["condition_a_identifier"])
            types.append("single-read") # TODO: clarify "why???"
        for sample in condition_b_samples:
            samples.append(path.basename(sample).rsplit(".")[0])
            conditions.append(config["condition_b_identifier"])
            types.append("single-read")

        df = pd.DataFrame(OrderedDict([('sample', samples),('condition', conditions),('type', types)]))
        df.to_csv(output.coldata, sep="\t", index=False)

rule write_de_params:
    input:
    output:
        de_params = "de_analysis/de_params.tsv"
    run:
        d = OrderedDict()
        d["Annotation"] = [config["annotation"]]
        d["min_samps_gene_expr"] = [config["min_samps_gene_expr"]]
        d["min_samps_feature_expr"] = [config["min_samps_feature_expr"]]
        d["min_gene_expr"] = [config["min_gene_expr"]]
        d["min_feature_expr"] = [config["min_feature_expr"]]
        df = pd.DataFrame(d)
        df.to_csv(output.de_params, sep="\t", index=False)

#rule de_analysis:
#    input:
#        de_params = rules.write_de_params.output.de_params,
#        coldata = rules.write_coldata.output.coldata,
#        tsv = rules.merge_counts.output,
#    output:
#        res_dge = "de_analysis/results_dge.tsv",
#        pdf_dge = "de_analysis/results_dge.pdf",
#        res_dtu_gene = "de_analysis/results_dtu_gene.tsv",
#        res_dtu_trs = "de_analysis/results_dtu_transcript.tsv",
#        res_dtu_stager = "de_analysis/results_dtu_stageR.tsv",
#        flt_counts = "merged/all_counts_filtered.tsv",
#        flt_counts_gens = "merged/all_gene_counts.tsv",
#    conda: "envs/env.yml"
#    script:
#        "scripts/de_analysis.R"
    
    
#TODO: make snakemake-compliant script
#rule plot_dtu_res:
#    input:
#        res_dtu_stager = "de_analysis/results_dtu_stageR.tsv",
#        flt_counts = "merged/all_counts_filtered.tsv",
#    output:
#        dtu_pdf = "de_analysis/dtu_plots.pdf",
#    conda: "envs/env.yml"
#    shell: """
#    scripts/plot_dtu_results.R
#    """

