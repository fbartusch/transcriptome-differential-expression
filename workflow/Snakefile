import fnmatch
from glob import glob
import os
from os import path
import pandas as pd
from collections import OrderedDict
import sys

localrules: merge_counts, write_coldata, write_de_params, de_analysis

include: "rules/utils.smk"
include: "rules/commons.smk"

configfile: "config.yml"

SNAKEDIR = path.dirname(workflow.snakefile)
inputdir  = config["inputdir"]
resdir = config["resdir"]

condition_a_samples = glob(os.path.join(inputdir, "*_{identifier}*".format(identifier=config["condition_a_identifier"])))

condition_b_samples = glob(os.path.join(inputdir, "*_{identifier}*".format(identifier=config["condition_b_identifier"])))

all_samples = condition_a_samples + condition_b_samples
datasets = [path.basename(x).rsplit(".")[0] for x in all_samples]

rule all:
    input:
        ver = rules.dump_versions.output.ver,
        count_tsvs = expand("counts/{sample}_salmon/quant.sf", sample=datasets),
        merged_tsv = "merged/all_counts.tsv",
        coldata = "de_analysis/coldata.tsv",
        de_params = "de_analysis/de_params.tsv",
        dispersion_graph = "de_analysis/dispersion_graph.svg",
        ma_graph         = "de_analysis/ma_graph.svg",
        de_heatmap       = "de_analysis/heatmap.svg",
        lfc_analysis     = "de_analysis/lfc_analysis.csv"

rule build_minimap_index: ## build minimap2 index
    input:
        genome = config["transcriptome"]
    output:
        index = "index/transcriptome_index.mmi"
    params:
        opts = config["minimap_index_opts"]
    conda: "envs/env.yml"
    shell:"""
        minimap2 -t {resources.cpus_per_task} {params.opts} -d {output.index} {input.genome}
    """

# mapping reads with minimap2
rule map_reads:
    input:
       index = rules.build_minimap_index.output.index,
       fastq = get_fastq_input(config["inputdir"])
    output:
       temp("alignments/{sample}.bam"),
    #log: "logs/minimap2/{sample}.log"
    params:
        opts = config["minimap2_opts"],
        msec = config["maximum_secondary"],
        psec = config["secondary_score_ratio"],
    conda: "envs/env.yml"
    shell:"""
    set -x
    echo $(pwd)
    T=/localscratch/$SLURM_JOB_ID ;
    cp ./{input.index} $T/. ;
    minimap2 -t {resources.cpus_per_task} -ax map-ont -p {params.psec} -N {params.msec} {params.opts} $T/$(basename ./{input.index}) {input.fastq} > {output}
    #| samtools view -Sb > {output}) 2> {log}
    """

rule sam_sort:
    input:
        sam = rules.map_reads.output
    output:
        temp("sorted_alignments/{sample}.bam")
    conda: "envs/env.yml"
    shell: "samtools sort -@ {resources.cpus_per_task} {input.sam} -o {output} -O bam"
    
rule sam_index:
    input:
        sbam = rules.sam_sort.output
    output:
        ibam = "sorted_alignments/{sample}_index.bam"
    conda: "envs/env.yml"
    shell:
        """
           samtools index -@ {resources.cpus_per_task} {input.sbam};
           mv {input.sbam} {output.ibam}
        """

rule count_reads:
    input:
        bam = rules.sam_index.output.ibam,
        trs = config["transcriptome"],
    output:
        tsv = "counts/{sample}_salmon/quant.sf",
    params:
        tsv_dir = "counts/{sample}_salmon",
        libtype = config["salmon_libtype"],
    conda: "envs/env.yml"
    shell: """
        salmon quant --noErrorModel -p {resources.cpus_per_task} -t {input.trs} -l {params.libtype} -a {input.bam} -o {params.tsv_dir}
    """

rule merge_counts:
    input:
        count_tsvs = expand("counts/{sample}_salmon/quant.sf", sample=datasets),
    output:
        "merged/all_counts.tsv"
    #conda: "envs/env.yml"
    script: 
        "scripts/merge_count_tsvs.py"

rule write_coldata:
    input:
    output:
        coldata = "de_analysis/coldata.tsv"
    run:
        samples, conditions, types = [], [], []
        for sample in condition_a_samples:
            samples.append(path.basename(sample).rsplit(".")[0])
            conditions.append(config["condition_a_identifier"])
            types.append("single-read") # TODO: clarify "why???"
        for sample in condition_b_samples:
            samples.append(path.basename(sample).rsplit(".")[0])
            conditions.append(config["condition_b_identifier"])
            types.append("single-read")

        df = pd.DataFrame(OrderedDict([('sample', samples),('condition', conditions),('type', types)]))
        df.to_csv(output.coldata, sep="\t", index=False)

rule write_de_params:
    input:
    output:
        de_params = "de_analysis/de_params.tsv"
    run:
        d = OrderedDict()
        d["Annotation"] = [config["annotation"]]
        d["min_samps_gene_expr"] = [config["min_samps_gene_expr"]]
        d["min_samps_feature_expr"] = [config["min_samps_feature_expr"]]
        d["min_gene_expr"] = [config["min_gene_expr"]]
        d["min_feature_expr"] = [config["min_feature_expr"]]
        df = pd.DataFrame(d)
        df.to_csv(output.de_params, sep="\t", index=False)

rule de_analysis:
    input:
        de_params = rules.write_de_params.output.de_params,
        coldata = rules.write_coldata.output.coldata,
        tsv = rules.merge_counts.output,
    output:
        dispersion_graph = "de_analysis/dispersion_graph.svg",
        ma_graph         = "de_analysis/ma_graph.svg",
        de_heatmap       = "de_analysis/heatmap.svg",
        de_top_heatmap   = "de_analysis/heatmap_top.svg"
        lfc_analysis     = "de_analysis/lfc_analysis.csv"
    threads: 4
    conda: "envs/env.yml"
    script:
        "scripts/de_analysis.py"
    
    
#TODO: make snakemake-compliant script
#rule plot_dtu_res:
#    input:
#        res_dtu_stager = "de_analysis/results_dtu_stageR.tsv",
#        flt_counts = "merged/all_counts_filtered.tsv",
#    output:
#        dtu_pdf = "de_analysis/dtu_plots.pdf",
#    conda: "envs/env.yml"
#    shell: """
#    scripts/plot_dtu_results.R
#    """

